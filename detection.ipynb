{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12dc5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#!pip install fastai -q --upgrade\n",
    "#!pip install pycocotools\n",
    "#!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5736d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import math\n",
    "\n",
    "#from fastai import *\n",
    "#from fastai.vision import *\n",
    "from fastai.vision.all import * \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpim\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as trans\n",
    "\n",
    "os.chdir(\"includes\")\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import albumentations as A\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ed3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gather ressources\n",
    "path=Path('images')\n",
    "testpath=Path('test')\n",
    "\n",
    "#get training/validation and test images\n",
    "images=get_image_files(path)\n",
    "testimages=get_image_files(testpath)\n",
    "\n",
    "#recover annotations from csv\n",
    "annotations=pd.read_csv(path/'wappen.csv')\n",
    "annotations=annotations.drop(['file_size','file_attributes','region_count','region_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3734f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mpl helpers\n",
    "def printImg(image,ax=None,size=None):\n",
    "    if ax==None:\n",
    "        im,ax=plt.subplots(figsize=size)\n",
    "        \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.imshow(image)\n",
    "    return ax\n",
    "        \n",
    "def drawBB(ax, bl, tr,col):\n",
    "    height=tr[1]-bl[1]\n",
    "    width=tr[0]-bl[0]\n",
    "    bb=patches.Rectangle(bl,width,height,fill=False,color=col,lw=2.0)\n",
    "    ax.add_patch(bb)\n",
    "    \n",
    "def drawText(ax,x,y,text,col):\n",
    "    ax.text(x,y,text,color=col,fontsize=14)\n",
    "    \n",
    "def printImages(images,annotation_dict):\n",
    "    for image in images:\n",
    "        ax=printImg(mpim.imread(image),size=[14,14])\n",
    "        imgname=os.path.normpath(image).split(os.sep)[-1]\n",
    "        annotations=annotation_dict[imgname]\n",
    "        for bb,label in annotations:\n",
    "            color=colordict[label]\n",
    "            drawBB(ax,(bb[0],bb[1]),(bb[2],bb[3]),color)\n",
    "            #drawText(ax,(bb[2]+bb[0])//2-8*len(label),(bb[3]+bb[1])//2-14,label,color)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0646ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some global lookup stuff\n",
    "colordict={\"wappen\":'magenta',\"text\":'cyan',\"#na#\":'green',\"objekt\":'red'}\n",
    "clsToId={\"wappen\":1,\"text\":2,\"objekt\":1}\n",
    "IdToClsSeperated={1:\"wappen\",2:\"text\"}\n",
    "IdToClsMerged={1:\"objekt\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5ee6c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global structures\n",
    "\n",
    "#transforms\n",
    "mytransforms = A.Compose([\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    #A.Rotate(limit=5),\n",
    "    A.Resize(1024,1024)\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.1, label_fields=['labels']))\n",
    "\n",
    "testTransforms=trans.Compose([\n",
    "    trans.Resize((1024,1024)),\n",
    "    trans.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd1f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset creation helpers\n",
    "def selectClosestText(filename,bbox,maxYdown=100,maxYup=800,maxXdiff=300):\n",
    "    #collect all text boxes that are located closely to the top of the wappen box or None if there is none\n",
    "    maxY=min(bbox[1],bbox[3])\n",
    "    medX=(bbox[0]+bbox[2])/2\n",
    "    closestBox=None\n",
    "    closestDist=0\n",
    "    for tbox,cls in imtoann[filename]:\n",
    "        if cls!='text':\n",
    "            continue\n",
    "        mdlY=(tbox[1]+tbox[3])/2\n",
    "        if (mdlY-maxY)>maxYdown:\n",
    "            continue\n",
    "        if (maxY-mdlY)>maxYup:\n",
    "            continue\n",
    "        if abs((bbox[0]-tbox[0]))>maxXdiff and abs((bbox[2]-tbox[2]))>maxXdiff:\n",
    "            continue\n",
    "        mdlX=(tbox[0]+tbox[2])/2\n",
    "        dist=(maxY-mdlY)**2 +(medX-mdlX)**2\n",
    "        if closestBox==None:\n",
    "            closestBox=(tbox,cls)\n",
    "            closestDist=dist\n",
    "        elif dist<closestDist:\n",
    "            closestBox=(tbox,cls)\n",
    "            closestDist=dist\n",
    "            \n",
    "    return closestBox\n",
    "\n",
    "#transforms a given dictionary to a dictionary that is needed to create torch datasets\n",
    "def getDatasetDictionary(inDict):\n",
    "    imtoannlist={}\n",
    "    for image in inDict:\n",
    "        bblist=[]\n",
    "        lbllist=[]\n",
    "        for bb,lbl in inDict[image]:\n",
    "            bblist.append(bb)\n",
    "            lbllist.append(lbl)\n",
    "        imtoannlist[image]=(bblist,lbllist)\n",
    "    return imtoannlist\n",
    "\n",
    "def createTorchDataset(dataset,val_pct=0.2):\n",
    "    #split indices\n",
    "    indices=torch.randperm(len(dataset)).tolist()\n",
    "    \n",
    "    lastTrain=len(dataset)-(int(len(dataset)*val_pct))\n",
    "\n",
    "    #create troch datasets\n",
    "    trainingSet=torch.utils.data.Subset(dataset, indices[:lastTrain])\n",
    "    validationSet=torch.utils.data.Subset(dataset, indices[lastTrain:])\n",
    "    \n",
    "    return (trainingSet,validationSet)\n",
    "\n",
    "def summarizeData(inDict):\n",
    "    numImgs=len(inDict)\n",
    "    numDict={}\n",
    "    for key in inDict:\n",
    "        for j in range(len(inDict[key][0])):\n",
    "            labels=inDict[key][1]\n",
    "            for i in range(len(labels)):\n",
    "                if(labels[i] in numDict):\n",
    "                    numDict[labels[i]]+=1\n",
    "                else:\n",
    "                    numDict[labels[i]]=1\n",
    "            \n",
    "    print(\"Dataset comprises \"+str(numImgs)+\" images\")\n",
    "    for key in numDict:\n",
    "        print(\"Number of \"+str(key)+\": \"+str(numDict[key]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107e401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the dataset class\n",
    "class EmblemTextSet(torch.utils.data.Dataset):\n",
    "    def __init__(self,images,annotation_dict,transforms=None):\n",
    "        self.transforms=transforms\n",
    "        self.images=images\n",
    "        self.dict=annotation_dict\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(images[idx]).convert(\"RGB\")\n",
    "        imgkey=os.path.normpath(images[idx]).split(os.sep)[-1]\n",
    "        boxes=torch.tensor(self.dict[imgkey][0])\n",
    "        labels=torch.tensor([clsToId[label] for label in self.dict[imgkey][1]],dtype=torch.int64)    \n",
    "       \n",
    "        if self.transforms!=None:\n",
    "            img_transformed = self.transforms(image=np.array(img), bboxes=boxes,labels=labels)\n",
    "        \n",
    "        imgt=trans.ToTensor()(img_transformed['image'])\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.tensor(img_transformed['bboxes'],dtype=torch.float32)\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"]=torch.tensor([(b[3]-b[1])*(b[2]-b[0]) for b in target['boxes']])\n",
    "        target[\"iscrowd\"]=torch.tensor([0]*len(target['labels']), dtype=torch.int64)\n",
    "    \n",
    "        return imgt, target\n",
    "    \n",
    "    def __len__(self):\n",
    "         return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48ecf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train functions\n",
    "\n",
    "#train function that stops training process when the precision on the validation set decreases or stagnates\n",
    "def train_model_prevent_overfit(model,datasets,bs,epochs,filename=\"./stored_models/currentModelEpochFinder\",optimizer=None,lr=0.005,\n",
    "                               max_decrease=0.3,max_worsening_epochs=10):\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    #create loaders\n",
    "    train,valid=datasets\n",
    "    train_loader=torch.utils.data.DataLoader(train, batch_size=bs, shuffle=True,collate_fn=utils.collate_fn)\n",
    "    validation_loader=torch.utils.data.DataLoader(valid, batch_size=bs, shuffle=True,collate_fn=utils.collate_fn)\n",
    "      \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    if optimizer==None:\n",
    "        optimizer = torch.optim.SGD(params, lr=lr,momentum=0.9,weight_decay=0.0005)\n",
    "        #optimizer =torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "        \n",
    "    lr_scheduler =torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=4, T_mult=2)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "    \n",
    "    bestmAp=0.0\n",
    "    mAphistory=[]\n",
    "    strikes=0\n",
    "    \n",
    "    #training loop\n",
    "    for epoch in range(epochs):\n",
    "        #train on training set\n",
    "        train_one_epoch(model, optimizer, train_loader, device, epoch,\n",
    "                   print_freq=10)\n",
    "        #adjust lr\n",
    "        lr_scheduler.step()\n",
    "  \n",
    "        #check valid set\n",
    "        coco_evaluator=evaluate(model, validation_loader, device=device)\n",
    "        \n",
    "        mAp=0\n",
    "        for _,evaluator in coco_evaluator.coco_eval.items():\n",
    "            mAp=sum(evaluator.stats[0:3])/3\n",
    "            \n",
    "        mAphistory.append(mAp)\n",
    "            \n",
    "        if(mAp<bestmAp*(1-max_decrease)):\n",
    "            print(\"precision decreased by more than \"+str(max_decrease)+\" percent from best value at epoch: \"+str(epoch)+\" abandoning!\")\n",
    "            break\n",
    "        \n",
    "        if(mAp>bestmAp):\n",
    "            strikes=0\n",
    "            bestmAp=mAp\n",
    "            torch.save(model.state_dict(),filename+\"-bestmap\")\n",
    "        else:\n",
    "            strikes+=1\n",
    "            \n",
    "        if(strikes>=max_worsening_epochs):\n",
    "            print(\"precision on validation has not increased in \"+str(max_worsening_epochs)+\" epochs in epoch \"+str(epoch)+\" abandoning!\")\n",
    "            break\n",
    "        \n",
    "    #store trained parameters\n",
    "    torch.save(model.state_dict(),filename)\n",
    "    #return the epoch number with the best loss on validation set\n",
    "    print(mAphistory)\n",
    "    return mAphistory.index(max(mAphistory))+1\n",
    "    \n",
    "\n",
    "\n",
    "#default train function\n",
    "def train_model(model,datasets,bs,epochs,filename=\"./stored_models/currentModel\",optimizer=None,lr=0.005):\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    #create loaders\n",
    "    train,valid=datasets\n",
    "    train_loader=torch.utils.data.DataLoader(train, batch_size=bs, shuffle=True,collate_fn=utils.collate_fn)\n",
    "    validation_loader=torch.utils.data.DataLoader(valid, batch_size=bs, shuffle=True,collate_fn=utils.collate_fn)\n",
    "      \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    if optimizer==None:\n",
    "        optimizer = torch.optim.SGD(params, lr=lr,momentum=0.9,weight_decay=0.0005)\n",
    "        #optimizer =torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "        \n",
    "    lr_scheduler =torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=4, T_mult=2)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.1)\n",
    "    \n",
    "    #training loop\n",
    "    for epoch in range(epochs):\n",
    "        #train on training set\n",
    "        train_one_epoch(model, optimizer, train_loader, device, epoch,\n",
    "                   print_freq=10)\n",
    "        #adjust lr\n",
    "        lr_scheduler.step()\n",
    "  \n",
    "        #check valid set\n",
    "        evaluate(model, validation_loader, device=device)\n",
    "        \n",
    "    #store trained parameters\n",
    "    torch.save(model.state_dict(),filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09139655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shameless copy of train_one_epoch from torchvision engine, with slight adjustments \n",
    "#such that loss and learning rate values are stored in respective lists\n",
    "\n",
    "def train_one_epoch_LRfind(model, optimizer, data_loader, device, epoch, \n",
    "                           lr_updater,print_freq,lrlist,losslist,stopFactor,lrfactor):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
    "    \n",
    "    #this loops all minibatches\n",
    "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        \n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            #print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            #print(loss_dict_reduced)\n",
    "            #sys.exit(1)\n",
    "            return\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #append (smoothed) current loss to losslist\n",
    "        \n",
    "        curloss=float(losses.cpu().detach().numpy())\n",
    "        if len(losslist)>0:\n",
    "            losslist.append(0.05  * curloss + (1 - 0.05) * losslist[-1])\n",
    "        else:\n",
    "            losslist.append(curloss)\n",
    "\n",
    "        curlr=optimizer.state_dict()[\"param_groups\"][0][\"lr\"] \n",
    "        lrlist.append(curlr)\n",
    "        \n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        lr_updater.step()\n",
    "            \n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "#plots learning rate vs loss\n",
    "def plotLR(lrates,losses,skipf=1,skipl=1,minimal=None,steepest=None):\n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    plt.plot(lrates[skipf:-skipl],losses[skipf:-skipl])\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"learning rate\")\n",
    "    plt.ylabel(\"smoothened loss\")\n",
    "    if(minimal!=None):\n",
    "        plt.scatter([minimal[0]],[minimal[1]])\n",
    "    if(steepest!=None):\n",
    "        plt.scatter([steepest[0]],[steepest[1]])\n",
    "    plt.show()\n",
    "\n",
    "#learning rate finder, if optimizer is passed, it is expected to have lowerBound as learning rate\n",
    "def findStartingLR(data,model,bs,lowerBound=1e-7,upperBound=0.1,stopFactor=10,optimizer=None,steps=100,plot=True):\n",
    "    #load model to device\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "    model.to(device)\n",
    "    \n",
    "    train_loader=torch.utils.data.DataLoader(data[0], batch_size=bs, shuffle=True,collate_fn=utils.collate_fn)\n",
    "    \n",
    "    curLR=lowerBound\n",
    "    \n",
    "    lrates=[]\n",
    "    losses=[]\n",
    "\n",
    "    #set optimizer with initial lr at lower bound\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    if optimizer==None:\n",
    "        optimizer=torch.optim.SGD(params, lr=lowerBound)\n",
    "        \n",
    "    #set lr scheduler to a lambda function such that lr gets increased exponentially from lower bound to upper bound\n",
    "    totalfactor=upperBound/lowerBound\n",
    "    factor=totalfactor**(1.0/steps)\n",
    "    multiplyLR= lambda x: factor**x\n",
    "    multiplyLR_sched=torch.optim.lr_scheduler.LambdaLR(optimizer, multiplyLR)\n",
    "    \n",
    "    #set number of epochs to include at least steps number of steps\n",
    "    batches_per_epoch=math.ceil(len(data[0])/bs)\n",
    "    num_epochs=math.ceil(steps/batches_per_epoch)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        train_one_epoch_LRfind(model, optimizer, train_loader, device, i, lr_updater=multiplyLR_sched, print_freq=10,\n",
    "                               lrlist=lrates,losslist=losses,stopFactor=stopFactor,lrfactor=factor)\n",
    "        \n",
    "    losses.pop(0)\n",
    "    lrates.pop(0)\n",
    "    \n",
    "    #find min point\n",
    "    mind=losses.index(min(losses))\n",
    "    minimal=(lrates[mind],min(losses))\n",
    "    \n",
    "    descents=[]\n",
    "    lrates_descents=[]\n",
    "    for i in range(1,len(losses)):\n",
    "        descents.append(losses[i-1]-losses[i])\n",
    "        lrates_descents.append(lrates[i-1])\n",
    "    \n",
    "    steepestind=descents.index(max(descents))\n",
    "    steepest=(lrates_descents[steepestind],0.5*(losses[steepestind]+losses[steepestind+1]))\n",
    "    \n",
    "    #print(lrates)\n",
    "    #print(losses)\n",
    "    #print(lrates_descents)\n",
    "    #print(descents)\n",
    "    \n",
    "    #plot LR vs losses\n",
    "    if plot:\n",
    "        plotLR(lrates,losses,minimal=minimal,steepest=steepest)\n",
    "    \n",
    "    #return minimal and steepest point\n",
    "    return (steepest[0],minimal[0])\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deaae98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains a frcnn model with resnet50 backbone on the given data\n",
    "#if a state dict is given, the model parameters will be initialized according to it\n",
    "#otherwise a pretrained model on CoCo is used\n",
    "def singleTrainingSession(data,num_classes,bs,stateDict=None,filename=\"./stored_models/currentModel\"):\n",
    "    #create trained model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
    "    \n",
    "    #create the model that is used for finding the lr\n",
    "    lrfindmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    lrfindmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes)\n",
    "    \n",
    "    #load previously trained parameters\n",
    "    if stateDict!=None:\n",
    "        model.load_state_dict(torch.load(stateDict))\n",
    "        lrfindmodel.load_state_dict(torch.load(stateDict))\n",
    "    \n",
    "    #use lrfindmodel to get a decent learning rate for sgd\n",
    "    lrsuggestions=findStartingLR(data,lrfindmodel,bs=bs,lowerBound=1e-7,upperBound=1,steps=200,plot=False)\n",
    "    print(lrsuggestions)\n",
    "    \n",
    "    #in cases where the steepest descent is at a higher lr than the minimal loss, no reasonable learning rate could be discovered\n",
    "    if(lrsuggestions[0]>lrsuggestions[1]):\n",
    "        print(\"No reasonable learning rate could be discovered\")\n",
    "        return\n",
    "    \n",
    "    #take a value that is close to the steepest descent shifted a bit towards the minimal loss\n",
    "    lr=(995/1000*lrsuggestions[0]+5/1000*lrsuggestions[1])\n",
    "    \n",
    "    #train_model(model,data,bs=bs,epochs=20,filename=filename,lr=lr)\n",
    "    bestNepochs=train_model_prevent_overfit(model,data,bs,epochs=200,filename=filename,lr=lr,max_decrease=0.1,max_worsening_epochs=10)\n",
    "    print(\"best results after \"+str(bestNepochs)+\" epochs\")\n",
    "    \n",
    "    #return the trained model\n",
    "    return model\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f13c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters a list of annotations based on their score, by looking at a models confidence in the predictions\n",
    "def filerConfidenceGap(boxes,labels,confidences,always_include_above=0.9,never_include_below=0.2):\n",
    "    #find distances between confidences to discover largest gap\n",
    "    largest_gap=0\n",
    "    gap_confidence=0\n",
    "    for i in range(1,len(confidences)):\n",
    "        gap=confidences[i]-confidences[i-1]\n",
    "        if(gap>largest_gap):\n",
    "            largest_gap=gap\n",
    "            gap_confidence=0.5*(confidences[i]+confidences[i-1])\n",
    "   \n",
    "    #if largest gap < lower limit, set it to the lower limit\n",
    "    #if largest gap > upper limit, set it to the upper limit\n",
    "    gap_confidence=min(max(gap_confidence,never_include_below),always_include_above)\n",
    "    \n",
    "    fboxes=[]\n",
    "    flabels=[]\n",
    "    fconfidences=[]\n",
    "    #filter\n",
    "    for i in range(len(confidences)):\n",
    "        if confidences[i]>gap_confidence:\n",
    "            fboxes.append([float(boxes[i][0]),float(boxes[i][1]),float(boxes[i][2]),float(boxes[i][3])])\n",
    "            flabels.append(int(labels[i]))\n",
    "            fconfidences.append(confidences[i])\n",
    "    \n",
    "    return (fboxes,flabels,fconfidences)\n",
    "\n",
    "def evaluateAndPrintData(model,dataset,labelDict,max_num=4,filename=\"./stored_models/currentModel\",score_threshold=0.5):\n",
    "    #load state dict from path\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    #put model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(min(max_num,len(dataset))):\n",
    "        image,gtannotations=dataset[i]\n",
    "        #print(mpim.imread(images[0]))\n",
    "        \n",
    "        img=Image.fromarray(image.mul(255).permute(1, 2,0).byte().numpy())\n",
    "        ax=printImg(np.asarray(img),size=[14,14])\n",
    "       \n",
    "        annotations=(gtannotations['boxes'],gtannotations['labels'])\n",
    "        #print ground truth\n",
    "        for j in range(len(annotations[1])):\n",
    "            label=labelDict[int(annotations[1][j])]\n",
    "            bb=annotations[0][j]\n",
    "            color=colordict[label]\n",
    "            drawBB(ax,(bb[0],bb[1]),(bb[2],bb[3]),color)\n",
    "            #drawText(ax,(bb[2]+bb[0])//2-8*len(label),(bb[3]+bb[1])//2-14,label,color)\n",
    "        \n",
    "        prediction=model([image])\n",
    "        ax=printImg(np.asarray(img),size=[14,14])\n",
    "        \n",
    "        boxes=prediction[0][\"boxes\"]\n",
    "        labels=prediction[0][\"labels\"]\n",
    "        scores=prediction[0][\"scores\"]\n",
    "        #print(scores)\n",
    "       \n",
    "        #filter out low scoring predictions\n",
    "        fboxes,flabels,_=filerConfidenceGap(boxes,labels,scores,always_include_above=0.9,never_include_below=score_threshold)\n",
    "        \n",
    "        #print(fboxes)\n",
    "        #print(flabels)\n",
    "        annotations=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "        \n",
    "        #print prediction\n",
    "        for j in range(len(annotations[1])):\n",
    "            label=labelDict[int(annotations[1][j])]\n",
    "            bb=annotations[0][j]\n",
    "            color=colordict[label]\n",
    "            drawBB(ax,(bb[0],bb[1]),(bb[2],bb[3]),color)\n",
    "            #drawText(ax,(bb[2]+bb[0])//2-8*len(label),(bb[3]+bb[1])//2-14,label,color)\n",
    "\n",
    "#evaluate unseen image with model and print the resulting predicted boxes\n",
    "def evaluateAndPrintUnseenData(model,images,transforms,labelDict,max_num=4,filename=\"./stored_models/currentModel\",score_threshold=0.5):\n",
    "    #load state dict from path\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    #put model in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(min(max_num,len(images))):\n",
    "        img=Image.open(images[i]).convert(\"RGB\")\n",
    "        imgTensor=transforms(img)\n",
    "        #get prediction\n",
    "        prediction=model([imgTensor])\n",
    "        #get image\n",
    "        image=Image.fromarray(imgTensor.mul(255).permute(1, 2,0).byte().numpy())\n",
    "        ax=printImg(np.asarray(image),size=[14,14])\n",
    "        \n",
    "        boxes=prediction[0][\"boxes\"]\n",
    "        labels=prediction[0][\"labels\"]\n",
    "        scores=prediction[0][\"scores\"]\n",
    "        \n",
    "        #filter out low scoring predictions\n",
    "        fboxes=[]\n",
    "        flabels=[]\n",
    "        for j in range(len(boxes)):\n",
    "            if scores[j]>score_threshold:\n",
    "                fboxes.append([boxes[j][0],boxes[j][1],boxes[j][2],boxes[j][3]])\n",
    "                flabels.append(labels[j])\n",
    "                \n",
    "        annotations=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "        \n",
    "        #print prediction\n",
    "        for j in range(len(annotations[1])):\n",
    "            label=labelDict[int(annotations[1][j])]\n",
    "            bb=annotations[0][j]\n",
    "            color=colordict[label]\n",
    "            drawBB(ax,(bb[0],bb[1]),(bb[2],bb[3]),color)\n",
    "            #drawText(ax,(bb[2]+bb[0])//2-8*len(label),(bb[3]+bb[1])//2-14,label,color)\n",
    "\n",
    "#prints evaluations on seen and unseen data\n",
    "def summary(model,images,data,transforms,labelDict,max_n=4,filename=\"./stored_models/currentModel\",score_threshold=0.5):\n",
    "    evaluateAndPrintData(model,data,labelDict,max_num=max_n,filename=filename,score_threshold=score_threshold)\n",
    "    evaluateAndPrintUnseenData(model,images,transforms,labelDict,max_num=max_n,filename=filename,score_threshold=score_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d455f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#more functions to display images \n",
    "def area(box1,box2):\n",
    "    x1,y1,x2,y2=box1\n",
    "    x3,y3,x4,y4=box2\n",
    "    xdiff=min(max(x1,x2),max(x3,x4))-max(min(x1,x2),min(x3,x4))\n",
    "    ydiff=min(max(y1,y2),max(y3,y4))-max(min(y1,y2),min(y3,y4))\n",
    "    \n",
    "    if(xdiff<0 or ydiff<0):\n",
    "        return 0\n",
    "    \n",
    "    return xdiff*ydiff\n",
    "\n",
    "def IoU(box1,box2):\n",
    "    x1,y1,x2,y2=box1\n",
    "    x3,y3,x4,y4=box2\n",
    "    \n",
    "    area1=(max(x1,x2)-min(x1,x2))*(max(y1,y2)-min(y1,y2))\n",
    "    area2=(max(x3,x4)-min(x3,x4))*(max(y3,y4)-min(y3,y4))\n",
    "    \n",
    "    intersectArea=area(box1,box2)\n",
    "    unitedArea=area1+area2-intersectArea\n",
    "    \n",
    "    IoU=0.\n",
    "    if(unitedArea>0):\n",
    "            IoU=intersectArea/unitedArea\n",
    "            \n",
    "    return IoU\n",
    "\n",
    "#check weather first box is completely surrounded by second box\n",
    "def fullSurround(box1,box2):\n",
    "    x1,y1,x2,y2=box1\n",
    "    x3,y3,x4,y4=box2\n",
    "    if x3<x1 and y3<y1 and x4>x2 and y4>y2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "#filter boxes of the same class that overlap too much\n",
    "def filterOverlappingBoxes(boxes,label,scores,threshold=0.9):\n",
    "    filteredBoxes=[]\n",
    "    filteredLabels=[]\n",
    "    filteredOut=[]\n",
    "    for i in range(len(boxes)):\n",
    "        conflict=False\n",
    "        for j in range(len(boxes)):\n",
    "            if i==j:\n",
    "                continue\n",
    "            if label[i]!=label[j]:\n",
    "                continue\n",
    "            if j in filteredOut:\n",
    "                continue       \n",
    "            #check for overlap with box j\n",
    "            overlap=IoU(boxes[i],boxes[j])\n",
    "            if overlap<threshold and not fullSurround(boxes[i],boxes[j]):\n",
    "                continue\n",
    "            #if overlap is big enough, and confidence in second box is higher, mark for removal\n",
    "            if scores[i]<scores[j]:\n",
    "                conflict=True\n",
    "                break\n",
    "        \n",
    "        if conflict:\n",
    "            filteredOut.append(i)\n",
    "        else:\n",
    "            filteredBoxes.append(boxes[i])\n",
    "            filteredLabels.append(label[i])\n",
    "        \n",
    "    return (filteredBoxes,filteredLabels)\n",
    "\n",
    "\n",
    "\n",
    "#convert bounding box predictions trained on a 1024x1024 to original image size\n",
    "def convertBoxesToOriginalSize(boxes,image,transformedSize=(1024,1024)):\n",
    "    img=Image.open(image).convert(\"RGB\")\n",
    "    origSize=img.size\n",
    "    #print(transformedSize[0])\n",
    "    #print(origSize[0])\n",
    "    scales=(origSize[0]/transformedSize[0] , origSize[1]/transformedSize[1])\n",
    "    scaledBoxes=[]\n",
    "    for box in boxes:\n",
    "        scaledBoxes.append([scales[0]*box[0],scales[1]*box[1],scales[0]*box[2],scales[1]*box[3]])\n",
    "    return scaledBoxes\n",
    "\n",
    "\n",
    "#print predictions from two different models next to each other for comparison\n",
    "def displayComparison(images,models,labelDicts,size=(6,4),annotations=None,gtLabels=None,score_min=0.3,score_max=0.9,threshold=0.9):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    #get number of rows/columns for subplots\n",
    "    height=len(images)\n",
    "    cols=len(models)\n",
    "    \n",
    "    if(height>=10):\n",
    "        half=len(images)//2\n",
    "        displayComparison(images[:half],models,labelDicts,size,annotations,gtLabels,score_min,score_max,threshold)\n",
    "        displayComparison(images[half:],models,labelDicts,size,annotations,gtLabels,score_min,score_max,threshold)\n",
    "        return\n",
    "   \n",
    "    #set up overlying figure\n",
    "    #fig=plt.figure(figsize=size)\n",
    "    fig,axarr=plt.subplots(height,cols,figsize=(size[0]*cols,size[1]*height))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #loop through all subplots\n",
    "    for row in range(height):\n",
    "        image=images[row]\n",
    "        img=Image.open(image).convert(\"RGB\")\n",
    "        for col in range(cols):\n",
    "            index=row*cols+col+1\n",
    "               \n",
    "            #print annotated ground truth image\n",
    "            if(annotations!=None and col==cols-1):\n",
    "                ax=showImg(mpim.imread(image),fig,height,cols,index,size)\n",
    "                imgname=os.path.normpath(image).split(os.sep)[-1]\n",
    "                annList=annotations[imgname]\n",
    "                for bb,label in annList:\n",
    "                    color=gtLabels[label]\n",
    "                    drawBB(ax,(bb[0],bb[1]),(bb[2],bb[3]),color)\n",
    "            \n",
    "            #display predicted image\n",
    "            else:\n",
    "                model=models[col]\n",
    "                #transform\n",
    "                imgTensor=testTransforms(img)\n",
    "                #image=Image.fromarray(imgTensor.mul(255).permute(1, 2,0).byte().numpy())\n",
    "                #predict\n",
    "                prediction=model([imgTensor])\n",
    "                #filter\n",
    "                fboxes,flabels,fconfidences=filerConfidenceGap(prediction[0][\"boxes\"],prediction[0][\"labels\"],prediction[0][\"scores\"],always_include_above=score_max,never_include_below=score_min)\n",
    "                #print(fboxes)\n",
    "                #print(flabels)\n",
    "                fboxes,flabels=filterOverlappingBoxes(fboxes,flabels,fconfidences,threshold=threshold)\n",
    "                \n",
    "                fboxes=convertBoxesToOriginalSize(fboxes,image,transformedSize=(1024,1024))\n",
    "                annList=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "                \n",
    "                #print(annList[0])\n",
    "                ax=axarr[row,col]\n",
    "                #ax=plt.subplot(height,cols,index)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "                ax.imshow(img)\n",
    "                #ax.imshow(np.asarray(image))\n",
    "                #display\n",
    "                #ax=showImg(mpim.imread(image),fig,height,cols,index,size)\n",
    "                for i in range(len(annList[0])):\n",
    "                    bb=annList[0][i]\n",
    "                    label=annList[1][i]\n",
    "                    labeltxt=labelDicts[col][int(label)]\n",
    "                    color=colordict[labeltxt]\n",
    "                    drawBB(ax,(bb[0],bb[1]),(bb[2],bb[3]),color)\n",
    "\n",
    "                    \n",
    "#slight adaptation to the dataset variants of these methods to allow for      \n",
    "def selectClosestText2(boxes,label,index,maxYdown=100,maxYup=800,maxXdiff=300):\n",
    "    #collect all text boxes that are located closely to the top of the wappen box or None if there is none\n",
    "    bbox=boxes[index]\n",
    "    \n",
    "    maxY=min(bbox[1],bbox[3])\n",
    "    medX=(bbox[0]+bbox[2])/2\n",
    "    closestBox=None\n",
    "    closestDist=0\n",
    "    for i in range(0,len(boxes)):\n",
    "        tbox=boxes[i]\n",
    "        cls=label[i]\n",
    "        if i==index:\n",
    "            continue\n",
    "        if cls!=2:\n",
    "            continue\n",
    "        mdlY=(tbox[1]+tbox[3])/2\n",
    "        if (mdlY-maxY)>maxYdown:\n",
    "            continue\n",
    "        if (maxY-mdlY)>maxYup:\n",
    "            continue\n",
    "        if abs((bbox[0]-tbox[0]))>maxXdiff and abs((bbox[2]-tbox[2]))>maxXdiff:\n",
    "            continue\n",
    "        mdlX=(tbox[0]+tbox[2])/2\n",
    "        dist=(maxY-mdlY)**2 +(medX-mdlX)**2\n",
    "        if closestBox==None:\n",
    "            closestBox=tbox\n",
    "            closestDist=dist\n",
    "        elif dist<closestDist:\n",
    "            closestBox=tbox\n",
    "            closestDist=dist\n",
    "            \n",
    "    return closestBox        \n",
    "\n",
    "#return a list of merged\n",
    "def mergeBoxes(boxes,labels,maxYdown=100,maxYup=800,maxXdiff=300,mergedList=None):\n",
    "    mergedBoxes=[]\n",
    "    mergedLabels=[]\n",
    "    for i in range(len(boxes)):\n",
    "        x1,y1,x2,y2=boxes[i]\n",
    "        if labels[i]==1:\n",
    "            textbox=selectClosestText2(boxes,labels,i,maxYdown,maxYup,maxXdiff)\n",
    "            if textbox==None:\n",
    "                mergedBoxes.append(boxes[i])\n",
    "            else:\n",
    "                x3,y3,x4,y4=textbox\n",
    "                mergedBox=[min(x1,x3),min(y1,y3),max(x2,x4),max(y2,y4)]\n",
    "                mergedBoxes.append(mergedBox)\n",
    "            if mergedList!=None:\n",
    "                mergedList.append((boxes[i],textbox))\n",
    "            #new structure will always have the label 'object'\n",
    "            mergedLabels.append(1)\n",
    "            \n",
    "    return (mergedBoxes,mergedLabels)       \n",
    "\n",
    "\n",
    "#prints several steps next to each other from the post processing of seperated images\n",
    "def summarySeperated(model,images,labelDict,size=(12,10)):\n",
    "    n_steps=3 #3-steps prediction-overlapping box pruning-wappen,text merging\n",
    "    height=len(images)\n",
    "    if(height*size[1]>=2**16):\n",
    "        height=(2**16 -1)//size[1]\n",
    "    \n",
    "    \n",
    "    fig,axarr=plt.subplots(height,n_steps,figsize=(size[0]*n_steps,size[1]*height))\n",
    "    \n",
    "    for row in range(height):\n",
    "        #get predictions and postprocess\n",
    "        image=images[row]\n",
    "        img=Image.open(image).convert(\"RGB\")\n",
    "        imgTensor=testTransforms(img)\n",
    "        prediction=model([imgTensor])\n",
    "        fboxes,flabels,fconfidences=filerConfidenceGap(prediction[0][\"boxes\"],prediction[0][\"labels\"],prediction[0][\"scores\"])\n",
    "        fboxes=convertBoxesToOriginalSize(fboxes,image,transformedSize=(1024,1024))  \n",
    "        annList1=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "        fboxes,flabels=filterOverlappingBoxes(fboxes,flabels,fconfidences,threshold=0.2)\n",
    "        annList2=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "        fboxes,flabels=mergeBoxes(fboxes,flabels,maxYdown=100,maxYup=800,maxXdiff=300)\n",
    "        #print(fboxes)\n",
    "        #print(flabels)\n",
    "        annList3=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "        annLists=[annList1,annList2,annList3]\n",
    "        for col in range(n_steps):\n",
    "            #index=row*n_steps+col+1   \n",
    "            ax=axarr[row,col]\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            ax.imshow(img)\n",
    "            annList=annLists[col]\n",
    "            for i in range(len(annList[0])):\n",
    "                bb=annList[0][i]\n",
    "                label=annList[1][i]\n",
    "                labeltxt=labelDict[int(label)]\n",
    "                if(col==2):\n",
    "                    labeltxt=\"objekt\"\n",
    "                color=colordict[labeltxt]\n",
    "                drawBB(ax,(bb[0],bb[1]),(bb[2],bb[3]),color)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ac4c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for outputting predicted image data to json\n",
    "def isIn(box,boxlist):\n",
    "    x1,y1,x2,y2=box\n",
    "    for box1,box2 in boxlist:\n",
    "        x3,y3,x4,y4=box1\n",
    "        if(x1==x3 and x2==x4 and y1==y3 and y2==y4):\n",
    "            return True\n",
    "        if(box2==None):\n",
    "            continue\n",
    "        x3,y3,x4,y4=box2\n",
    "        if(x1==x3 and x2==x4 and y1==y3 and y2==y4):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "#create a json file for image and \n",
    "def createJson(image,model,fname):\n",
    "    \n",
    "    imgname=os.path.normpath(image).split(os.sep)[-1]\n",
    "    data={}\n",
    "    op=open(fname,\"w+\")\n",
    "    data[\"filename\"]=imgname\n",
    "    data[\"regions\"]=[]\n",
    "      \n",
    "    img=Image.open(image).convert(\"RGB\")\n",
    "    imgTensor=testTransforms(img)\n",
    "    prediction=model([imgTensor])\n",
    "       \n",
    "    fboxes,flabels,fconfidences=filerConfidenceGap(prediction[0][\"boxes\"],prediction[0][\"labels\"],prediction[0][\"scores\"])\n",
    "    fboxes=convertBoxesToOriginalSize(fboxes,image,transformedSize=(1024,1024))  \n",
    "    fboxes,flabels=filterOverlappingBoxes(fboxes,flabels,fconfidences,threshold=0.2)\n",
    "    annList1=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "    mergedList=[]\n",
    "    fboxes,flabels=mergeBoxes(fboxes,flabels,maxYdown=100,maxYup=800,maxXdiff=300,mergedList=mergedList)\n",
    "    annList2=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "    \n",
    "    for i in range (len(annList2[0])):\n",
    "        curdict={}\n",
    "        curdict[\"class\"]=\"merged\"\n",
    "        box=annList2[0][i]\n",
    "        text=mergedList[i][1]\n",
    "        symbol=mergedList[i][0]\n",
    "        curdict[\"box\"]=[float(box[0]),float(box[1]),float(box[2]),float(box[3])]\n",
    "        if text!=None:\n",
    "            curdict[\"textbox\"]=[float(text[0]),float(text[1]),float(text[2]),float(text[3])]\n",
    "        curdict[\"symbolbox\"]=[float(symbol[0]),float(symbol[1]),float(symbol[2]),float(symbol[3])]\n",
    "        data[\"regions\"].append(curdict)\n",
    "        \n",
    "    #take care of the artifacts that were not merged\n",
    "    for j in range(len(annList1[0])):\n",
    "        box=annList1[0][i]\n",
    "        cls=annList1[1][i]\n",
    "        if not isIn(box,mergedList):\n",
    "            curdict={}\n",
    "            if(cls==1):\n",
    "                curdict[\"class\"]=\"symbol\"\n",
    "            if(cls==2):\n",
    "                curdict[\"class\"]=\"text\"\n",
    "            curdict[\"box\"]=[float(box[0]),float(box[1]),float(box[2]),float(box[3])]\n",
    "            data[\"regions\"].append(curdict)\n",
    "    \n",
    "    #print(data)\n",
    "    json.dump(data,op)\n",
    "    \n",
    "    \n",
    "#create a distinct json file for each region of intrest in image\n",
    "def createIndividualJsons(image,model,fname):\n",
    "    imgname=os.path.normpath(image).split(os.sep)[-1]\n",
    "    \n",
    "    img=Image.open(image).convert(\"RGB\")\n",
    "    imgTensor=testTransforms(img)\n",
    "    prediction=model([imgTensor])\n",
    "    \n",
    "    \n",
    "    fboxes,flabels,fconfidences=filerConfidenceGap(prediction[0][\"boxes\"],prediction[0][\"labels\"],prediction[0][\"scores\"])\n",
    "    fboxes=convertBoxesToOriginalSize(fboxes,image,transformedSize=(1024,1024))  \n",
    "    fboxes,flabels=filterOverlappingBoxes(fboxes,flabels,fconfidences,threshold=0.2)\n",
    "    annList1=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "    mergedList=[]\n",
    "    fboxes,flabels=mergeBoxes(fboxes,flabels,maxYdown=100,maxYup=800,maxXdiff=300,mergedList=mergedList)\n",
    "    annList2=(torch.tensor(fboxes),torch.tensor(flabels))\n",
    "    \n",
    "    #print(annList2[0])\n",
    "    #print(mergedList)\n",
    "    \n",
    "    coveredParts=[]\n",
    "    #loop through every merged box\n",
    "    for i in range (len(annList2[0])):\n",
    "        bbox=annList2[0][i]\n",
    "        data={}\n",
    "        outfile=fname+\"-merged-\"+str(i)+\".json\"\n",
    "        op=open(outfile,\"w+\")\n",
    "        data[\"filename\"]=imgname\n",
    "        data[\"region\"]=[]\n",
    "        curdict={}\n",
    "        curdict[\"class\"]=\"merged\"\n",
    "        curdict[\"bbox\"]=[float(bbox[0]),float(bbox[1]),float(bbox[2]),float(bbox[3])]\n",
    "        data[\"region\"].append(curdict)\n",
    "        originSymbol=mergedList[i][0]\n",
    "        curdict={}\n",
    "        curdict[\"class\"]=\"symbol\"\n",
    "        curdict[\"bbox\"]=[float(originSymbol[0]),float(originSymbol[1]),float(originSymbol[2]),float(originSymbol[3])]\n",
    "        data[\"region\"].append(curdict)\n",
    "        originText=mergedList[i][1]\n",
    "        if originText!=None:\n",
    "            curdict={}\n",
    "            curdict[\"class\"]=\"text\"\n",
    "            curdict[\"bbox\"]=[float(originText[0]),float(originText[1]),float(originText[2]),float(originText[3])]\n",
    "            data[\"region\"].append(curdict)\n",
    "        #print(\"\")\n",
    "        #print(data)\n",
    "        json.dump(data,op)\n",
    "    \n",
    "    idx=0\n",
    "    for i in range (len(annList1[0])):\n",
    "        box=annList1[0][i]\n",
    "        cls=annList1[1][i]\n",
    "        #search if box was already covered as part of the merged boxes\n",
    "        if not isIn(box,mergedList):\n",
    "            data={}\n",
    "            outfile=fname+\"-fragment-\"+str(idx)+\".json\"\n",
    "            op=open(outfile,\"w+\")\n",
    "            idx+=1\n",
    "            data[\"filename\"]=imgname\n",
    "            data[\"region\"]=[]\n",
    "            curdict={}\n",
    "            if(cls==1):\n",
    "                curdict[\"class\"]=\"symbol\"\n",
    "            if(cls==2):\n",
    "                curdict[\"class\"]=\"text\"\n",
    "            curdict[\"bbox\"]=[float(box[0]),float(box[1]),float(box[2]),float(box[3])]\n",
    "            data[\"region\"].append(curdict)\n",
    "            #print(\"\")\n",
    "            #print(data)\n",
    "            json.dump(data,op)\n",
    "\n",
    "\n",
    "#create json files for each image in images\n",
    "def createJsonsFor(model,images,paramfile=\"./stored_models/currentModel\",splitOnSymbols=False):\n",
    "    model.load_state_dict(torch.load(paramfile))\n",
    "    model.eval()\n",
    "    for image in images:\n",
    "        imgname=os.path.normpath(image).split(os.sep)[-1]\n",
    "        fname=\"./outputfiles/\"+imgname.split('.')[0]\n",
    "        if splitOnSymbols:\n",
    "            createIndividualJsons(image,model,fname)\n",
    "        else:\n",
    "            createJson(image,model,fname+\".json\")\n",
    "       \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bb05f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary from image to a list of annotations\n",
    "imtoann={}\n",
    "for index,row in annotations.iterrows():\n",
    "    imgname=row['filename']\n",
    "    #print(imgname)\n",
    "    bbdata=json.loads(row['region_shape_attributes'])\n",
    "    #print(bbdata)\n",
    "    labeldata=json.loads(row['region_attributes'])\n",
    "    x1=int(bbdata['x'])\n",
    "    y1=int(bbdata['y'])\n",
    "    x2=x1+int(bbdata['width'])\n",
    "    y2=y1+int(bbdata['height'])\n",
    "    bb=[x1,y1,x2,y2]\n",
    "    label=labeldata['region']\n",
    "    annotation=(bb,label)\n",
    "    if imgname in imtoann:\n",
    "        imtoann[imgname].append(annotation)\n",
    "    else:\n",
    "        imtoann[imgname]=[annotation]\n",
    "        \n",
    "imtoannlist=getDatasetDictionary(imtoann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b58064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of merged bbs\n",
    "imToMergedBB={}\n",
    "for item in imtoann:\n",
    "    filename=item\n",
    "    for item in imtoann[filename]:\n",
    "        bbox,cls=item\n",
    "        if cls!='wappen':\n",
    "            continue\n",
    "        matchingText=selectClosestText(filename,bbox)\n",
    "        #print(item)\n",
    "        #print(matchingText)\n",
    "        \n",
    "        annotation=None\n",
    "        if matchingText==None:\n",
    "            annotation=(bbox,'objekt')\n",
    "        else:\n",
    "            txtbox=matchingText[0]\n",
    "            minX=min(bbox[0],bbox[2],txtbox[0],txtbox[2])\n",
    "            minY=min(bbox[1],bbox[3],txtbox[1],txtbox[3])\n",
    "            maxX=max(bbox[0],bbox[2],txtbox[0],txtbox[2])\n",
    "            maxY=max(bbox[1],bbox[3],txtbox[1],txtbox[3])\n",
    "            annotation=([minX,minY,maxX,maxY],'objekt')\n",
    "        if(filename in imToMergedBB):\n",
    "            imToMergedBB[filename].append(annotation)\n",
    "        else:\n",
    "            imToMergedBB[filename]=[annotation] \n",
    "            \n",
    "imToAnnMerged=getDatasetDictionary(imToMergedBB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63bde362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets\n",
    "#box and text seperated\n",
    "datasetSeperated=EmblemTextSet(images,imtoannlist,mytransforms)\n",
    "dataSeperated=createTorchDataset(datasetSeperated,0.2)\n",
    "#box and text merged\n",
    "datasetMerged=EmblemTextSet(images,imToAnnMerged,mytransforms)\n",
    "dataMerged=createTorchDataset(datasetMerged,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fa2319f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#findLRSeperated\n",
    "testmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = testmodel.roi_heads.box_predictor.cls_score.in_features\n",
    "testmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features,3)\n",
    "\n",
    "findStartingLR(dataSeperated,testmodel,bs=4,lowerBound=1e-7,upperBound=1,stopFactor=10,optimizer=None,steps=200,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4e510c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#findLRMerged\n",
    "testmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = testmodel.roi_heads.box_predictor.cls_score.in_features\n",
    "testmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features,2)\n",
    "\n",
    "findStartingLR(dataMerged,testmodel,bs=4,lowerBound=1e-7,upperBound=1,stopFactor=10,optimizer=None,steps=200,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b075dcfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainedModelSeperated=singleTrainingSession(dataSeperated,3,bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e33aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModelMerged=singleTrainingSession(dataMerged,2,bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a75ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize seperated\n",
    "testmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = testmodel.roi_heads.box_predictor.cls_score.in_features\n",
    "testmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features,3)\n",
    "summary(testmodel,testimages,datasetSeperated,testTransforms,IdToClsSeperated,max_n=10,filename=\"./stored_models/modelSeperated\",score_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8876a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize merged\n",
    "testmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = testmodel.roi_heads.box_predictor.cls_score.in_features\n",
    "testmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features,2)\n",
    "summary(testmodel,testimages,datasetMerged,testTransforms,IdToClsMerged,max_n=10,filename=\"./stored_models/modelMerged\",score_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20692d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSeperated = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = modelSeperated.roi_heads.box_predictor.cls_score.in_features\n",
    "modelSeperated.roi_heads.box_predictor = FastRCNNPredictor(in_features,3)\n",
    "modelSeperated.load_state_dict(torch.load(\"./stored_models/modelSeperated\"))\n",
    "\n",
    "modelMerged = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = modelMerged.roi_heads.box_predictor.cls_score.in_features\n",
    "modelMerged.roi_heads.box_predictor = FastRCNNPredictor(in_features,2)\n",
    "modelMerged.load_state_dict(torch.load(\"./stored_models/modelMerged\"))\n",
    "\n",
    "displayComparison(testimages,[modelSeperated,modelMerged],[IdToClsSeperated,IdToClsMerged],size=(14,10),annotations=None,gtLabels=None,threshold=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40f9fd11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarySeperated(modelSeperated,testimages,IdToClsSeperated,size=(12,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7c234fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "createJsonsFor(modelSeperated,testimages,paramfile=\"./stored_models/modelSeperated\",splitOnSymbols=False)#for one file per image\n",
    "#createJsonsFor(modelSeperated,testimages,paramfile=\"./stored_models/modelSeperated\",splitOnSymbols=False) #for one file per symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25022ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset comprises 77 images\n",
      "Number of wappen: 17935\n",
      "Number of text: 19748\n"
     ]
    }
   ],
   "source": [
    "summarizeData(imtoannlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12849c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd3977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
